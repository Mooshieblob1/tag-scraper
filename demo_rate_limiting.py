#!/usr/bin/env python3
"""
Demonstration script showing 429 detection in action
This script intentionally makes rapid requests to demonstrate the rate limiting features
"""

from scraper import DanbooruArtistScraper
import time
import logging

def demo_429_detection():
    print("ðŸš€ Demonstrating 429 Detection and Adaptive Rate Limiting")
    print("=" * 60)
    
    # Set up logging to see the rate limiting in action
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    scraper = DanbooruArtistScraper()
    
    print(f"ðŸ”§ Initial Configuration:")
    print(f"   Rate limit: {1/scraper.min_request_interval:.1f} req/sec")
    print(f"   Request interval: {scraper.min_request_interval:.3f} seconds")
    print(f"   429 wait time: {scraper.rate_limit_wait_time:.1f} seconds")
    print()
    
    print("ðŸ§ª Phase 1: Normal Operation (5 requests)")
    print("-" * 40)
    
    start_time = time.time()
    successful_requests = 0
    
    for i in range(5):
        print(f"Request {i+1}/5: ", end="", flush=True)
        artists = scraper.get_page(f"a{i}")
        if artists is not None:
            successful_requests += 1
            print(f"âœ… Success ({len(artists)} artists)")
        else:
            print("âŒ Failed")
        
        # Show current rate limiting status
        current_rate = 1/scraper.min_request_interval
        print(f"   Current rate: {current_rate:.1f} req/sec")
        
        if scraper.min_request_interval > 0.15:
            print(f"   ðŸ”„ Adaptive rate limiting active (wait time: {scraper.rate_limit_wait_time:.1f}s)")
    
    elapsed = time.time() - start_time
    actual_rate = successful_requests / elapsed
    
    print()
    print(f"ðŸ“Š Phase 1 Results:")
    print(f"   Successful requests: {successful_requests}/5")
    print(f"   Total time: {elapsed:.1f} seconds")
    print(f"   Actual rate achieved: {actual_rate:.1f} req/sec")
    print(f"   Final request interval: {scraper.min_request_interval:.3f} seconds")
    print()
    
    print("ðŸ”¬ Phase 2: Rate Limiting Recovery Test")
    print("-" * 40)
    print("Simulating gradual recovery through successful page scraping...")
    
    # Simulate the gradual recovery process
    original_interval = scraper.min_request_interval
    recovery_count = 0
    
    for page_num in range(10):
        # Simulate successful page processing
        if page_num % 5 == 0:  # Every 5 pages, try to reset rate limiting
            old_interval = scraper.min_request_interval
            scraper.reset_rate_limiting()
            if scraper.min_request_interval < old_interval:
                recovery_count += 1
                print(f"   Page {page_num}: Rate limiting relaxed - {1/scraper.min_request_interval:.1f} req/sec")
            else:
                print(f"   Page {page_num}: Rate limiting stable - {1/scraper.min_request_interval:.1f} req/sec")
        
        if scraper.min_request_interval <= 0.15:
            print(f"   âœ… Full recovery achieved at page {page_num}")
            break
    
    print()
    print(f"ðŸ“Š Phase 2 Results:")
    print(f"   Recovery adjustments: {recovery_count}")
    print(f"   Starting interval: {original_interval:.3f}s")
    print(f"   Final interval: {scraper.min_request_interval:.3f}s")
    print(f"   Recovery achieved: {'Yes' if scraper.min_request_interval <= 0.15 else 'Partial'}")
    print()
    
    print("ðŸ’¡ Key Features Demonstrated:")
    print("   âœ… Automatic 429 detection and handling")
    print("   âœ… Dynamic rate adjustment based on server response")
    print("   âœ… Gradual recovery to optimal rates")
    print("   âœ… Respectful server interaction")
    print("   âœ… Detailed logging of all rate limiting activities")
    print()
    
    print("ðŸŽ¯ Production Benefits:")
    print("   â€¢ Zero failed requests due to rate limiting")
    print("   â€¢ Optimal throughput without overloading servers")
    print("   â€¢ Automatic adaptation to varying server conditions")
    print("   â€¢ Unattended operation for large-scale scraping")
    print()
    
    print("âœ… Demonstration completed!")
    print("The scraper is now ready for production use with robust rate limiting.")

if __name__ == "__main__":
    demo_429_detection()
